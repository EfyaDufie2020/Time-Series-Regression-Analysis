{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Understanding\n",
    "\n",
    "Problem Statement\n",
    "\n",
    "Project Goal\n",
    "Stakeholders\n",
    "Key Metrics\n",
    "Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis\n",
    "\n",
    "### Business Questions\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "2. hich dates have the lowest and highest sales for each year (excluding days the store was closed)?\n",
    "3. Compare the sales for each month across the years and determine which month of which year had the highest sales.\n",
    "4. Did the earthquake impact sales?\n",
    "5. Are certain stores or groups of stores selling more products? (Cluster, city, state, type)\n",
    "6. Are sales affected by promotions, oil prices and holidays?\n",
    "7. What analysis can we get from the date and its extractable features?\n",
    "8. Which product family and stores did the promotions affect.\n",
    "9. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "10. Does the payment of wages in the public sector on the 15th and last days of the month influence the store sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment variables management\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database connection\n",
    "import pyodbc\n",
    "import MySQLdb\n",
    "import mysql.connector\n",
    "import pymysql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, display_html\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling and utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import calendar\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "# import joblib\n",
    "\n",
    "# Data fetching\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Suppressing warnings to avoid cluttering the output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load environment variables from.env file into dictionary\n",
    "environment_variables = dotenv_values(\".env\")\n",
    " \n",
    "# get the values for the environment variables\n",
    "server = environment_variables.get(\"server\")\n",
    "login = environment_variables.get(\"login\")\n",
    "password = environment_variables.get(\"password\")\n",
    "database = environment_variables.get(\"database\")\n",
    " \n",
    "# Create a database connection string using pyodbc\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={login};PWD={password}\"\n",
    "#Establish a connection to the database\n",
    "try:\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    print(\"Connection successful:\", connection_string)    \n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the SQL query to show specific tables in the database\n",
    "db_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM INFORMATION_SCHEMA.TABLES\n",
    "        WHERE TABLE_SCHEMA = 'dbo'\n",
    "        \"\"\"\n",
    "# Read data from the SQL query result into a DataFrame using the established database connection\n",
    "schema_df = pd.read_sql(db_query, connection)\n",
    " \n",
    "#  Check whether data has been retrieved successfully to confirm successful connection to database\n",
    "try:\n",
    "    schema_df = pd.read_sql(db_query, connection)    \n",
    "    print(\"Data retrieved successfully\")\n",
    "    print()\n",
    "    print(schema_df)    \n",
    "except Exception as e:\n",
    "    print(\"Failed to retrieve data:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the SQL query to show specific tables in the database\n",
    "db_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM stores        \n",
    "        \"\"\"\n",
    "# Read data from the SQL query result into a DataFrame using the established database connection\n",
    "df_stores = pd.read_sql(db_query, connection)\n",
    " \n",
    "# Display the DataFrame\n",
    "df_stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the SQL query to show specific tables in the database\n",
    "db_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM oil        \n",
    "        \"\"\"\n",
    "# Read data from the SQL query result into a DataFrame using the established database connection\n",
    "df_oil = pd.read_sql(db_query, connection)\n",
    " \n",
    "# Display the DataFrame\n",
    "df_oil.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the SQL query to show specific tables in the database\n",
    "db_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM holidays_events        \n",
    "        \"\"\"\n",
    "# Read data from the SQL query result into a DataFrame using the established database connection\n",
    "df_holidays = pd.read_sql(db_query, connection)\n",
    " \n",
    "# Display the DataFrame\n",
    "df_holidays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL of the file to download\n",
    "url = \"https://github.com/EfyaDufie2020/Career_Accelerator_LP3-Regression/raw/main/store-sales-forecasting.zip\"\n",
    " \n",
    "# Local file path where the file will be saved\n",
    "local_file_path = '../Data/store-sales-forecasting.zip'\n",
    " \n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    " \n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    " \n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Write the content of the response to the specified file path\n",
    "    with open(local_file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"File downloaded successfully\")\n",
    "   \n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(local_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.dirname(local_file_path))\n",
    "    print(\"File extracted successfully\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the downloaded CSV file into a DataFrame\n",
    "df_train = pd.read_csv('../Data/train.csv')\n",
    " \n",
    "# Display the DataFrame\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the downloaded CSV file into a DataFrame\n",
    "df_transactions = pd.read_csv('../Data/transactions.csv')\n",
    " \n",
    "# Display the DataFrame\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the downloaded CSV file into a DataFrame\n",
    "df_test = pd.read_csv('../Data/test.csv')\n",
    " \n",
    "# Display the DataFrame\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the downloaded CSV file into a DataFrame\n",
    "df_sample = pd.read_csv('../Data/sample_submission.csv')\n",
    " \n",
    "# Display the DataFrame\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the common columns ('store_nbr' and 'date') in the datasets using the inner merge() function\n",
    "# Merge train_data with stores_df based on 'store_nbr' \n",
    "columnmerged_df1 = pd.merge(df_train,df_stores, on='store_nbr', how='left')\n",
    "columnmerged_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge merged_df1 with trans_data based on 'date' and 'store_nbr' \n",
    "columnmerged_df2 = columnmerged_df1.merge(df_transactions, on=['date', 'store_nbr'], how='inner')\n",
    "columnmerged_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge merged_df2 with holidays_events_df based on 'date' \n",
    "columnmerged_df3 = columnmerged_df2.merge(df_holidays, on='date', how='inner')\n",
    "columnmerged_df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge merged_df3 with oil_df based on 'date' \n",
    "columndata= columnmerged_df3.merge(df_oil, on='date', how='inner') \n",
    "\n",
    "columndata.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columndata.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the common columns ('store_nbr' and 'date') in the datasets using the inner merge() function\n",
    "# Merge train_data with stores_df based on 'store_nbr' columnmerged_df1 = pd.merge(df_train,stores, on='store_nbr', how='left')\n",
    "#merged_df1.head(3)\n",
    "# Merge merged_df1 with trans_data based on 'date' and 'store_nbr' columnsmerged_df2 = merged_df1.merge(df_trx, on=['date', 'store_nbr'], how='inner')\n",
    "# #merged_df2.head(3)\n",
    "# Merge merged_df2 with holidays_events_df based on 'date' columnmerged_df3 = merged_df2.merge(holidays_events, on='date', how='inner')\n",
    "#merged_df3.head(3)\n",
    " # Merge merged_df3 with oil_df based on 'date' columndata= merged_df3.merge(oil, on='date', how='inner')data.head(3)\n",
    " # View the first five rows of the merged dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
